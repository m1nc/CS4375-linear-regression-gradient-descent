{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m1nc/CS4375-linear-regression-gradient-descent/blob/main/part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYikCItKsnzw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset links\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "\n",
        "df = pd.read_csv(url, sep=\";\")"
      ],
      "metadata": {
        "id": "3zitB8CLwCUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "\n",
        "# Drop nulls or any NA values\n",
        "df = df.dropna()\n",
        "\n",
        "# Drop any redundant rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Convert categorical variables to numerical variables\n",
        "# All of the values in our Wine dataset contains only numerical attributes so conversion from categorical to numerical was not required.\n",
        "\n",
        "# If you feel an attribute is not suitable or is not correlated with the outcome, you might want to get rid of it.\n",
        "# Our dataset doesn't have any obvious junk columns.\n",
        "\n",
        "# Standardizing the features using StandardScaler of sklearn\n",
        "# Splitting the features and target\n",
        "\n",
        "# Features\n",
        "X = df.iloc[:, :-1].values\n",
        "\n",
        "# Target\n",
        "Y = df.iloc[:, -1].values\n",
        "\n",
        "# Standardizing features\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "MO1J4OndxNlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and testing parts\n",
        "# Train/Test ratio is 80/20\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "0XYVqh2NHTsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing parameters\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "weights = np.zeros(n_features)\n",
        "bias = 0\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Prediction function\n",
        "\n",
        "def predict(X, weights, bias):\n",
        "  return np.dot(X, weights) + bias\n",
        "\n",
        "# MSE\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "  return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Gradient descent\n",
        "\n",
        "def gradient_descent(X, Y, weights, bias, learning_rate, iterations):\n",
        "  n = len(Y)\n",
        "  log = []\n",
        "\n",
        "  for i in range(iterations):\n",
        "    y_pred = predict(X, weights, bias)\n",
        "\n",
        "    error = y_pred - Y\n",
        "    dw = (1/n) * np.dot(X.T, error)\n",
        "    db = (1/n) * np.sum(error)\n",
        "\n",
        "    weights = weights - (learning_rate * dw)\n",
        "    bias = bias - (learning_rate * db)\n",
        "\n",
        "    mse = mean_squared_error(Y, predict(X, weights, bias))\n",
        "    log.append((i, learning_rate, mse))\n",
        "\n",
        "  return weights, bias, log\n"
      ],
      "metadata": {
        "id": "5G3gyEWzOxxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter turning\n",
        "\n",
        "learning_rates = [0.001, 0.002, 0.005]\n",
        "iterations = 1000\n",
        "\n",
        "best_mse = float(\"inf\")\n",
        "best_params =  None\n",
        "all_logs = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "  w,b, log = gradient_descent(\n",
        "      X_train,\n",
        "      Y_train,\n",
        "      weights.copy(),\n",
        "      bias,\n",
        "      lr,\n",
        "      iterations\n",
        "  )\n",
        "\n",
        "  final_mse = log[-1][2]\n",
        "\n",
        "  if final_mse < best_mse:\n",
        "    best_mse = final_mse\n",
        "    best_params = (lr, w, b)"
      ],
      "metadata": {
        "id": "It_oii-QRHc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving log file\n",
        "\n",
        "with open(\"training_log.txt\", \"w\") as f:\n",
        "  f.write(\"iteration, learning_rate, iterations, mse\")\n",
        "\n",
        "  for entry in all_logs:\n",
        "    f.write(f\"{entry[0]}, {entry[1]}, {entry[2]}, {entry[3]}\")"
      ],
      "metadata": {
        "id": "E59kLL6AYWmX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}